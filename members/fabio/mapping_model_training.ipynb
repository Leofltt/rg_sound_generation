{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "mapping_model_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcAvS4amzDrZ"
      },
      "source": [
        "# Mapping model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0fVn8yUJl_v"
      },
      "source": [
        "## Setup Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m33xuTjEKazJ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn7CQ4GQizHy"
      },
      "source": [
        "## Install Dependencies\n",
        "\n",
        "First we install the required dependencies with `pip`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjhdKFJbvRVU"
      },
      "source": [
        "%tensorflow_version 2.x\r\n",
        "!pip install -qU ddsp[data_preparation]==1.0.1"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LVV4Dc61HHY"
      },
      "source": [
        "## Make directories to save model and data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XJcymGj1IwY"
      },
      "source": [
        "import os\r\n",
        "\r\n",
        "drive_dir = '/content/drive/My Drive/nsynth_guitar'\r\n",
        "checkpoint_dir = os.path.join(drive_dir, 'mapping/checkpoint')\r\n",
        "\r\n",
        "assert os.path.exists(drive_dir)\r\n",
        "print('Drive Directory Exists:', drive_dir)\r\n",
        "\r\n",
        "!mkdir -p \"$checkpoint_dir\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fgGZzyMGyA4"
      },
      "source": [
        "## Clear existing checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYaZoeNeGrvo"
      },
      "source": [
        "# import shutil\r\n",
        "\r\n",
        "# try:\r\n",
        "#     shutil.rmtree(checkpoint_dir)\r\n",
        "# except OSError as e:\r\n",
        "#     print(\"Error: %s : %s\" % (checkpoint_dir, e.strerror))"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxRUhnmKsUj9"
      },
      "source": [
        "### Download Complete NSynth Guitar Subset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTVOibF9sb3y"
      },
      "source": [
        "dataset_dir = '/content/complete'\r\n",
        "train_dataset_dir = os.path.join(dataset_dir, 'train')\r\n",
        "valid_dataset_dir = os.path.join(dataset_dir, 'valid')\r\n",
        "test_dataset_dir = os.path.join(dataset_dir, 'test')\r\n",
        "\r\n",
        "train_tfrecord_file = os.path.join(train_dataset_dir, 'complete.tfrecord')\r\n",
        "valid_tfrecord_file = os.path.join(valid_dataset_dir, 'complete.tfrecord')\r\n",
        "test_tfrecord_file = os.path.join(test_dataset_dir, 'complete.tfrecord')\r\n",
        "\r\n",
        "if not os.path.exists(dataset_dir):\r\n",
        "  train = 'https://osr-tsoai.s3.amazonaws.com/complete/train/complete.tfrecord'\r\n",
        "  valid = 'https://osr-tsoai.s3.amazonaws.com/complete/valid/complete.tfrecord'\r\n",
        "  test = 'https://osr-tsoai.s3.amazonaws.com/complete/test/complete.tfrecord'\r\n",
        "\r\n",
        "  print(\"Downloading train dataset to {}\\n\".format(train_dataset_dir))\r\n",
        "  !mkdir -p \"$train_dataset_dir\"\r\n",
        "  !curl $train --output $train_tfrecord_file\r\n",
        "\r\n",
        "  print(\"\\nDownloading valid dataset to {}\\n\".format(valid_dataset_dir))\r\n",
        "  !mkdir -p \"$valid_dataset_dir\"\r\n",
        "  !curl $valid --output $valid_tfrecord_file\r\n",
        "\r\n",
        "  print(\"\\nDownloading test dataset to {}\\n\".format(test_dataset_dir))\r\n",
        "  !mkdir -p \"$test_dataset_dir\"\r\n",
        "  !curl $test --output $test_tfrecord_file"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9AWf8NpBiB4"
      },
      "source": [
        "## Define DataProvider class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6180WP6AkkJ"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import ddsp.training.data as data\r\n",
        "\r\n",
        "class CompleteTFRecordProvider(data.RecordProvider):\r\n",
        "  def __init__(self,\r\n",
        "               file_pattern=None,\r\n",
        "               example_secs=4,\r\n",
        "               sample_rate=16000,\r\n",
        "               frame_rate=250,\r\n",
        "               map_func=None):\r\n",
        "    super().__init__(file_pattern, example_secs, sample_rate,\r\n",
        "                      frame_rate, tf.data.TFRecordDataset)\r\n",
        "    self._map_func = map_func\r\n",
        "\r\n",
        "  def get_dataset(self, shuffle=True):\r\n",
        "    def parse_tfexample(record):\r\n",
        "      features = tf.io.parse_single_example(record, self.features_dict)\r\n",
        "      if self._map_func is not None:\r\n",
        "        return self._map_func(features)\r\n",
        "      else:\r\n",
        "        return features\r\n",
        "\r\n",
        "    filenames = tf.data.Dataset.list_files(self._file_pattern, shuffle=shuffle)\r\n",
        "    dataset = filenames.interleave(\r\n",
        "        map_func=self._data_format_map_fn,\r\n",
        "        cycle_length=40,\r\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n",
        "    dataset = dataset.map(parse_tfexample,\r\n",
        "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n",
        "    return dataset\r\n",
        "\r\n",
        "  @property\r\n",
        "  def features_dict(self):\r\n",
        "    return {\r\n",
        "      'sample_name':\r\n",
        "        tf.io.FixedLenFeature([1], dtype=tf.string),\r\n",
        "      'note_number':\r\n",
        "        tf.io.FixedLenFeature([1], dtype=tf.int64),\r\n",
        "      'velocity':\r\n",
        "        tf.io.FixedLenFeature([1], dtype=tf.int64),\r\n",
        "      'instrument_source':\r\n",
        "        tf.io.FixedLenFeature([1], dtype=tf.int64),\r\n",
        "      'qualities':\r\n",
        "        tf.io.FixedLenFeature([10], dtype=tf.int64),\r\n",
        "      'audio':\r\n",
        "        tf.io.FixedLenFeature([self._audio_length], dtype=tf.float32),\r\n",
        "      'f0_hz':\r\n",
        "        tf.io.FixedLenFeature([self._feature_length], dtype=tf.float32),\r\n",
        "      'f0_confidence':\r\n",
        "        tf.io.FixedLenFeature([self._feature_length], dtype=tf.float32),\r\n",
        "      'loudness_db':\r\n",
        "        tf.io.FixedLenFeature([self._feature_length], dtype=tf.float32),\r\n",
        "      'f0_scaled':\r\n",
        "        tf.io.FixedLenFeature([self._feature_length], dtype=tf.float32),\r\n",
        "      'ld_scaled':\r\n",
        "        tf.io.FixedLenFeature([self._feature_length], dtype=tf.float32),\r\n",
        "      'z':\r\n",
        "        tf.io.FixedLenFeature([self._feature_length * 16], dtype=tf.float32),\r\n",
        "    }"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbUpwtyRB8wV"
      },
      "source": [
        "## Define features map function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbXhqrZaB5rw"
      },
      "source": [
        "def features_map(features):\r\n",
        "  note_number = features['note_number']\r\n",
        "  velocity = features['velocity']\r\n",
        "  instrument_source = features['instrument_source']\r\n",
        "  qualities = features['qualities']\r\n",
        "  f0_scaled = features['f0_scaled']\r\n",
        "  ld_scaled = features['ld_scaled']\r\n",
        "  z = features['z']\r\n",
        "\r\n",
        "  sequence_length = f0_scaled.shape[0]\r\n",
        "\r\n",
        "  def convert_to_sequence(feature):\r\n",
        "    channels = feature.shape[0]\r\n",
        "    feature = tf.expand_dims(feature, axis=0)\r\n",
        "\r\n",
        "    feature = tf.broadcast_to(feature, shape=(sequence_length, channels))\r\n",
        "    feature = tf.cast(feature, dtype=tf.float32)\r\n",
        "    \r\n",
        "    return feature\r\n",
        "\r\n",
        "  # Normalize data\r\n",
        "  # 0-127\r\n",
        "  note_number = note_number / 127\r\n",
        "  velocity = velocity / 127\r\n",
        "\r\n",
        "  # 0-2\r\n",
        "  # 0\tacoustic, 1\telectronic, 2\tsynthetic\r\n",
        "  instrument_source = instrument_source / 2\r\n",
        "\r\n",
        "  # Prepare dataset for a sequence to sequence mapping\r\n",
        "  note_number = convert_to_sequence(note_number)\r\n",
        "  velocity = convert_to_sequence(velocity)\r\n",
        "  instrument_source = convert_to_sequence(instrument_source)\r\n",
        "  qualities = convert_to_sequence(qualities)\r\n",
        "\r\n",
        "  f0_scaled = tf.expand_dims(f0_scaled, axis=-1)\r\n",
        "  ld_scaled = tf.expand_dims(ld_scaled, axis=-1)\r\n",
        "  z = tf.reshape(z, shape=(sequence_length, 16))\r\n",
        "\r\n",
        "  input = tf.concat(\r\n",
        "      [note_number, velocity, instrument_source, qualities, z],\r\n",
        "      axis=-1)\r\n",
        "  \r\n",
        "  output = tf.concat(\r\n",
        "      [f0_scaled, ld_scaled],\r\n",
        "      axis=-1)\r\n",
        "  \r\n",
        "  return (input, output)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7dYOU811Ni4"
      },
      "source": [
        "## Create datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBa055Xy1MIL"
      },
      "source": [
        "batch_size = 16\r\n",
        "example_secs = 4\r\n",
        "sample_rate = 16000\r\n",
        "frame_rate = 250\r\n",
        "\r\n",
        "# Create train dataset\r\n",
        "train_data_provider = CompleteTFRecordProvider(\r\n",
        "    file_pattern=train_tfrecord_file + '*',\r\n",
        "    example_secs=example_secs,\r\n",
        "    sample_rate=sample_rate,\r\n",
        "    frame_rate=frame_rate,\r\n",
        "    map_func=features_map)\r\n",
        "\r\n",
        "train_dataset = train_data_provider.get_batch(\r\n",
        "    batch_size,\r\n",
        "    shuffle=True,\r\n",
        "    repeats=-1)\r\n",
        "\r\n",
        "# Create valid dataset\r\n",
        "valid_data_provider = CompleteTFRecordProvider(\r\n",
        "    file_pattern=train_tfrecord_file + '*',\r\n",
        "    example_secs=example_secs,\r\n",
        "    sample_rate=sample_rate,\r\n",
        "    frame_rate=frame_rate,\r\n",
        "    map_func=features_map)\r\n",
        "\r\n",
        "valid_dataset = valid_data_provider.get_batch(\r\n",
        "    batch_size,\r\n",
        "    shuffle=True,\r\n",
        "    repeats=-1)\r\n",
        "\r\n",
        "# Create test dataset\r\n",
        "test_data_provider = CompleteTFRecordProvider(\r\n",
        "    file_pattern=train_tfrecord_file + '*',\r\n",
        "    example_secs=example_secs,\r\n",
        "    sample_rate=sample_rate,\r\n",
        "    frame_rate=frame_rate,\r\n",
        "    map_func=features_map)\r\n",
        "\r\n",
        "test_dataset = test_data_provider.get_batch(\r\n",
        "    batch_size,\r\n",
        "    shuffle=True,\r\n",
        "    repeats=-1)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTiP6aA82Uay"
      },
      "source": [
        "# Create and compile mapping model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26aSTwuy2ZKy"
      },
      "source": [
        "model = tf.keras.models.Sequential([\r\n",
        "    tf.keras.layers.GRU(32, return_sequences=True),\r\n",
        "    tf.keras.layers.Dense(2, activation='tanh')\r\n",
        "])\r\n",
        "\r\n",
        "loss = tf.keras.losses.MeanAbsoluteError()\r\n",
        "\r\n",
        "model.compile(\r\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\r\n",
        "    loss=loss,\r\n",
        "    metrics=[tf.keras.losses.MeanSquaredError()])"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zceUmkJI35zb"
      },
      "source": [
        "## Build model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoczioW23-Da"
      },
      "source": [
        "x_train, y_train = next(iter(train_dataset))\r\n",
        "out = model(x_train)\r\n",
        "\r\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SunnK0BY2utQ"
      },
      "source": [
        "# Load model checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi46si8f2bOi"
      },
      "source": [
        "checkpoint_file = os.path.join(checkpoint_dir, 'cp.ckpt')\r\n",
        "\r\n",
        "if os.path.isdir(checkpoint_dir) and os.listdir(checkpoint_dir):\r\n",
        "    model.load_weights(checkpoint_file)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkCyoCxp3l7r"
      },
      "source": [
        "## Create training callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23w7DNkh2ytf"
      },
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\r\n",
        "    filepath=checkpoint_file,\r\n",
        "    save_weights_only=True,\r\n",
        "    verbose=0,\r\n",
        "    save_freq='epoch')\r\n",
        "\r\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\r\n",
        "\r\n",
        "def scheduler(epoch, lr):\r\n",
        "  if epoch < 10:\r\n",
        "    return lr\r\n",
        "  else:\r\n",
        "    return lr * 0.9\r\n",
        "\r\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCkWXZD-5XsD"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xl4D5c_u5a1x"
      },
      "source": [
        "epochs = 20\r\n",
        "steps_per_epoch = 100\r\n",
        "validation_steps = 10\r\n",
        "\r\n",
        "model.fit(train_dataset,\r\n",
        "          epochs=epochs,\r\n",
        "          steps_per_epoch=steps_per_epoch,\r\n",
        "          validation_data=valid_dataset,\r\n",
        "          validation_steps=validation_steps,\r\n",
        "          callbacks=[checkpoint, early_stop, lr_scheduler])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNyLeJcN6wS8"
      },
      "source": [
        "## Evaluate model on test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX4SXagL6vwB"
      },
      "source": [
        "model.evaluate(test_dataset, steps=500)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}