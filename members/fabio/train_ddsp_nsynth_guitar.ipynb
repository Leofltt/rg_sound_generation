{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train_ddsp_nsynth_guitar.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcAvS4amzDrZ"
      },
      "source": [
        "# Train DDSP on NSynth guitar subset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0fVn8yUJl_v"
      },
      "source": [
        "## Setup Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m33xuTjEKazJ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn7CQ4GQizHy"
      },
      "source": [
        "## Install Dependencies\n",
        "\n",
        "First we install the required dependencies with `pip`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJzo4mdLYAXf"
      },
      "source": [
        "%tensorflow_version 2.x\r\n",
        "!pip install -qU ddsp[data_preparation]==1.0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIiPubo55cFQ"
      },
      "source": [
        "# import os\r\n",
        "\r\n",
        "# os.chdir(\"/content\")\r\n",
        "\r\n",
        "# ddsp_dir = \"ddsp\"\r\n",
        "# if not os.path.exists(ddsp_dir):\r\n",
        "#   !git clone https://github.com/magenta/$ddsp_dir\r\n",
        "\r\n",
        "# os.chdir(ddsp_dir)\r\n",
        "\r\n",
        "# %tensorflow_version 2.x\r\n",
        "# !pip install -e .\r\n",
        "# !pip install note_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LVV4Dc61HHY"
      },
      "source": [
        "## Make directories to save model and data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XJcymGj1IwY"
      },
      "source": [
        "import glob\r\n",
        "import os\r\n",
        "\r\n",
        "# create a symlink without spaces to avoid problems with apache-beam\r\n",
        "!ln -s '/content/drive/My Drive' '/gdrive'\r\n",
        "\r\n",
        "drive_dir = os.path.normpath('/gdrive/nsynth_guitar')\r\n",
        "drive_audio_dir = os.path.join(drive_dir, 'audio')\r\n",
        "drive_dataset_dir = os.path.join(drive_dir, 'dataset')\r\n",
        "drive_save_dir = os.path.join(drive_dir, 'save')\r\n",
        "\r\n",
        "assert os.path.exists(drive_dir)\r\n",
        "print('Drive Folder Exists:', drive_dir)\r\n",
        "\r\n",
        "# !mkdir -p \"$drive_audio_dir\"\r\n",
        "!mkdir -p \"$drive_dataset_dir\"\r\n",
        "!mkdir -p \"$drive_save_dir\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb4YD8woYD1H"
      },
      "source": [
        "## Prepare Dataset\n",
        "### Preprocess raw audio into TFRecord dataset\n",
        "\n",
        "We need to do some preprocessing on the raw audio you uploaded to get it into the correct format for training. This involves turning the full audio into short (4-second) examples, inferring the fundamental frequency (or \"pitch\") with [CREPE](http://github.com/marl/crepe), and computing the loudness. These features will then be stored in a sharded [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) file for easier loading. Depending on the amount of input audio, this process usually takes a few minutes.\n",
        "\n",
        "* (Optional) Transfer dataset from drive. If you've already created a dataset, from a previous run, this cell will skip the dataset creation step and copy the dataset from `$nsynth_guitar_dir/data` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsnkAHyHVrCW"
      },
      "source": [
        "drive_audio_files = glob.glob(drive_audio_dir + '/*')\n",
        "drive_dataset_files = glob.glob(drive_dataset_dir + '/*')\n",
        "drive_dataset_tfrecord = os.path.join(drive_dataset_dir, 'train.tfrecord')\n",
        "\n",
        "if not drive_dataset_files:\n",
        "  # Make a new dataset.\n",
        "  if not drive_audio_files:\n",
        "    raise ValueError('No audio files found.')\n",
        "\n",
        "  !ddsp_prepare_tfrecord \\\n",
        "    --input_audio_filepatterns=$drive_audio_files \\\n",
        "    --output_tfrecord_path=$drive_dataset_tfrecord \\\n",
        "    --num_shards=10 \\\n",
        "    --sliding_window_hop_secs=4 \\\n",
        "    --alsologtostderr\n",
        "\n",
        "# import tensorflow as tf\n",
        "# import ddsp.training.data_preparation.prepare_tfrecord_lib\n",
        "\n",
        "# if not drive_dataset_files:\n",
        "#   # Make a new dataset.\n",
        "#   if not drive_audio_files:\n",
        "#     raise ValueError('No audio files found.')\n",
        "\n",
        "#   input_audio_paths = []\n",
        "#   for filepattern in drive_audio_files:\n",
        "#     input_audio_paths.extend(tf.io.gfile.glob(filepattern))\n",
        "\n",
        "#   ddsp.training.data_preparation.prepare_tfrecord_lib.prepare_tfrecord(\n",
        "#       input_audio_paths,\n",
        "#       drive_dataset_tfrecord,\n",
        "#       num_shards=10,\n",
        "#       sample_rate=16000,\n",
        "#       frame_rate=250,\n",
        "#       window_secs=4.0,\n",
        "#       hop_secs=4.0,\n",
        "#       pipeline_options=['--runner=DirectRunner'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIsq0HrzbOF7"
      },
      "source": [
        "Let's load the dataset in the `ddsp` library and have a look at one of the examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA-FOmRgYdpZ"
      },
      "source": [
        "from ddsp.colab import colab_utils\n",
        "import ddsp.training\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "data_provider = ddsp.training.data.TFRecordProvider(drive_dataset_tfrecord)\n",
        "dataset = data_provider.get_dataset(shuffle=False)\n",
        "\n",
        "try:\n",
        "  ex = next(iter(dataset))\n",
        "except StopIteration:\n",
        "  raise ValueError(\n",
        "      'TFRecord contains no examples. Please try re-running the pipeline with '\n",
        "      'different audio file(s).')\n",
        "\n",
        "colab_utils.specplot(ex['audio'])\n",
        "colab_utils.play(ex['audio'])\n",
        "\n",
        "f, ax = plt.subplots(3, 1, figsize=(14, 4))\n",
        "x = np.linspace(0, 4.0, 1000)\n",
        "ax[0].set_ylabel('loudness_db')\n",
        "ax[0].plot(x, ex['loudness_db'])\n",
        "ax[1].set_ylabel('F0_Hz')\n",
        "ax[1].set_xlabel('seconds')\n",
        "ax[1].plot(x, ex['f0_hz'])\n",
        "ax[2].set_ylabel('F0_confidence')\n",
        "ax[2].set_xlabel('seconds')\n",
        "ax[2].plot(x, ex['f0_confidence'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gvXBa7PbuyY"
      },
      "source": [
        "## Train Model\n",
        "* Models typically perform well when the loss drops to the range of ~4.5-5.0.\n",
        "* Depending on the dataset this can take anywhere from 5k-30k training steps usually.\n",
        "* The default is set to 30k, but you can stop training at any time, and for timbre transfer, it's best to stop before the loss drops too far below ~5.0 to avoid overfitting.\n",
        "* On the colab GPU, this can take from around 3-20 hours. \n",
        "* By default, checkpoints will be saved every 300 steps with a maximum of 10 checkpoints (at ~60MB/checkpoint this is ~600MB). Feel free to adjust these numbers depending on the frequency of saves you would like and space on your drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpwQkSIKjEMZ"
      },
      "source": [
        "First, let's start up a [TensorBoard](https://www.tensorflow.org/tensorboard) to monitor our loss as training proceeds. \n",
        "\n",
        "Initially, TensorBoard will report `No dashboards are active for the current data set.`, but once training begins, the dashboards should appear."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2lx7yJneUXT"
      },
      "source": [
        "%reload_ext tensorboard\n",
        "import tensorboard as tb\n",
        "tb.notebook.start('--logdir \"{}\"'.format(drive_save_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT-8Koyvj46w"
      },
      "source": [
        "### We will now begin training. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poKO-mZEGYXZ"
      },
      "source": [
        "!ddsp_run \\\n",
        "  --mode=train \\\n",
        "  --alsologtostderr \\\n",
        "  --save_dir=\"$drive_save_dir\" \\\n",
        "  --gin_file=papers/iclr2020/nsynth_ae.gin \\\n",
        "  --gin_file=datasets/tfrecord.gin \\\n",
        "  --gin_param=\"TFRecordProvider.file_pattern='$drive_dataset_tfrecord'\" \\\n",
        "  --gin_param=\"batch_size=16\" \\\n",
        "  --gin_param=\"train_util.train.num_steps=30000\" \\\n",
        "  --gin_param=\"train_util.train.steps_per_save=300\" \\\n",
        "  --gin_param=\"trainers.Trainer.checkpoints_to_keep=10\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V95qxVjFzWR6"
      },
      "source": [
        "## Resynthesis\n",
        "\n",
        "Check how well the model reconstructs the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ5PPDZVzgFR"
      },
      "source": [
        "from ddsp.colab.colab_utils import play, specplot\n",
        "import ddsp.training\n",
        "import gin\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "data_provider = ddsp.training.data.TFRecordProvider(drive_dataset_tfrecord)\n",
        "dataset = data_provider.get_batch(batch_size=1, shuffle=False)\n",
        "\n",
        "try:\n",
        "  batch = next(iter(dataset))\n",
        "except OutOfRangeError:\n",
        "  raise ValueError(\n",
        "      'TFRecord contains no examples. Please try re-running the pipeline with '\n",
        "      'different audio file(s).')\n",
        "\n",
        "# Parse the gin config.\n",
        "gin_file = os.path.join(drive_save_dir, 'operative_config-0.gin')\n",
        "gin.parse_config_file(gin_file)\n",
        "\n",
        "# Load model\n",
        "model = ddsp.training.models.Autoencoder()\n",
        "model.restore(drive_save_dir)\n",
        "\n",
        "# Resynthesize audio.\n",
        "outputs = model(batch, training=False)\n",
        "audio_gen = model.get_audio_from_outputs(outputs)\n",
        "audio = batch['audio']\n",
        "\n",
        "print('Original Audio')\n",
        "specplot(audio)\n",
        "play(audio)\n",
        "\n",
        "print('Resynthesis')\n",
        "specplot(audio_gen)\n",
        "play(audio_gen)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}